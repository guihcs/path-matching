{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from om.ont import tokenize\n",
    "from py_stringmatching import SoftTfIdf, JaroWinkler\n",
    "from itertools import chain\n",
    "from pymagnitude import Magnitude\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_xml_paths(base_path, node, out):\n",
    "    current_label = node.tag.split('}')[-1]\n",
    "\n",
    "    if len(node) <= 0:\n",
    "        out.append(base_path + current_label)\n",
    "        for attribute in node.attrib:\n",
    "            out.append(base_path + current_label + '.' + attribute)\n",
    "\n",
    "    for child in node:\n",
    "        get_xml_paths(base_path + current_label + '.', child, out)\n",
    "\n",
    "\n",
    "def get_json_paths(base_path, node, out):\n",
    "    if type(node) is list:\n",
    "        for child in node:\n",
    "            get_json_paths(base_path, child, out)\n",
    "\n",
    "    elif type(node) is dict:\n",
    "        for child in node.keys():\n",
    "            get_json_paths(base_path + child + '.', node[child], out)\n",
    "    else:\n",
    "        out.append(base_path[:-1])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "root = ET.parse('./nat/odatis.xml').getroot()\n",
    "source = []\n",
    "get_xml_paths('', root, source)\n",
    "\n",
    "with open('./nat/aeris.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "target = []\n",
    "get_json_paths('', data, target)\n",
    "\n",
    "print(f'source paths count: {len(source)}, target paths count: {len(target)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "slist = []\n",
    "\n",
    "for q in chain(source, target):\n",
    "    slist.append(list(map(str.lower, tokenize(q))))\n",
    "\n",
    "soft_metric = SoftTfIdf(slist, sim_func=JaroWinkler().get_raw_score, threshold=0.8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def jaccard(a, b):\n",
    "    return len(a.intersection(b)) / len(a.union(b))\n",
    "\n",
    "\n",
    "for source_path in source:\n",
    "    n1 = list(map(str.lower, tokenize(source_path)))\n",
    "    s1 = set(n1)\n",
    "    for target_path in target:\n",
    "        n2 = list(map(str.lower, tokenize(target_path)))\n",
    "        s2 = set(n2)\n",
    "\n",
    "        data.append(['jaccard', source_path, target_path, jaccard(s1, s2)])\n",
    "        data.append(['soft_tf_idf', source_path, target_path, soft_metric.get_raw_score(n1, n2)])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<http://magnitude.plasticity.ai/glove/medium/glove.840B.300d.magnitude>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "glove = Magnitude(\"embs/glove.840B.300d.magnitude\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_vectors(paths):\n",
    "    elements = []\n",
    "    vectors = []\n",
    "\n",
    "    for path in paths:\n",
    "        unique_tokens = list(set(map(str.lower, tokenize(path))))\n",
    "        elements.append(path)\n",
    "        vectors.append(glove.query(unique_tokens).mean(0, keepdims=True))\n",
    "\n",
    "    return elements, np.concatenate(vectors)\n",
    "\n",
    "\n",
    "def get_np_similarity_matrix(v1, v2):\n",
    "    norm1 = np.linalg.norm(v1, axis=1, keepdims=True)\n",
    "    norm2 = np.linalg.norm(v2, axis=1, keepdims=True)\n",
    "\n",
    "    dot_prod = v1 @ v2.T\n",
    "    norm_matrix = norm1 * norm2.T\n",
    "\n",
    "    return dot_prod / norm_matrix\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "source_paths, source_vectors = get_vectors(source)\n",
    "target_paths, target_vectors = get_vectors(target)\n",
    "\n",
    "similarity = get_np_similarity_matrix(source_vectors, target_vectors)\n",
    "for i, source_path in enumerate(source_paths):\n",
    "    for j, target_path in enumerate(target_paths):\n",
    "        data.append(['glove', source_path, target_path, similarity[i, j]])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def encode_with_language_model(paths, tokenizer, model):\n",
    "    tokenized_paths = []\n",
    "\n",
    "    for path in paths:\n",
    "        tokenized_paths.append(' '.join(map(str.lower, tokenize(path))))\n",
    "\n",
    "    encoded_input = tokenizer(tokenized_paths, return_tensors='pt', padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        return model(**encoded_input)['pooler_output']\n",
    "\n",
    "\n",
    "def get_torch_similarity_matrix(v1, v2):\n",
    "    norm1 = v1.norm(dim=1, keepdim=True)\n",
    "    norm2 = v2.norm(dim=1, keepdim=True)\n",
    "\n",
    "    dot_prod = v1 @ v2.t()\n",
    "    norm_matrix = norm1 * norm2.t()\n",
    "\n",
    "    return dot_prod / norm_matrix\n",
    "\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert.eval()\n",
    "\n",
    "source_vectors = encode_with_language_model(source_paths, bert_tokenizer, bert)\n",
    "target_vectors = encode_with_language_model(target_paths, bert_tokenizer, bert)\n",
    "\n",
    "similarity = get_torch_similarity_matrix(source_vectors, target_vectors)\n",
    "\n",
    "for i, source_path in enumerate(source_paths):\n",
    "    for j, target_path in enumerate(target_paths):\n",
    "        data.append(['bert', source_path, target_path, similarity[i, j].item()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mini_lm_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "mini_lm = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "mini_lm.eval()\n",
    "\n",
    "source_vectors = encode_with_language_model(source_paths, mini_lm_tokenizer, mini_lm)\n",
    "target_vectors = encode_with_language_model(target_paths, mini_lm_tokenizer, mini_lm)\n",
    "\n",
    "similarity = get_torch_similarity_matrix(source_vectors, target_vectors)\n",
    "\n",
    "for i, source_path in enumerate(source_paths):\n",
    "    for j, target_path in enumerate(target_paths):\n",
    "        data.append(['all-MiniLM-L6-v2', source_path, target_path, similarity[i, j].item()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def metrics(correct, tries, total):\n",
    "    precision = 0 if tries == 0 else correct / tries\n",
    "    recall = 0 if total == 0 else correct / total\n",
    "    fm = 2 * (precision * recall) / (1 if precision + recall == 0 else precision + recall)\n",
    "    return precision, recall, fm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarity_table = pd.DataFrame(data, columns=['name', 'e1', 'e2', 'sim'])\n",
    "similarity_table.to_csv('./nat/path_sim.csv', index=False, encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarity_table.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_alignments = pd.read_csv('./nat/odatis-aeris.csv')\n",
    "\n",
    "alignments = set()\n",
    "\n",
    "for i, row in raw_alignments.iterrows():\n",
    "    alignments.add((row['key'], row['match']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "techniques = set(similarity_table['name'])\n",
    "similarity_map = dict()\n",
    "\n",
    "for threshold in tqdm(np.arange(0, 1.0, 0.05)):\n",
    "\n",
    "    for name in techniques:\n",
    "        filtered_rows = similarity_table.loc[(similarity_table['name'] == name) & (similarity_table['sim'] >= threshold)]\n",
    "\n",
    "        predicted = set()\n",
    "        for i, row in filtered_rows.iterrows():\n",
    "            predicted.add((row['e1'], row['e2']))\n",
    "\n",
    "        predicted_count = len(predicted)\n",
    "        correct_count = len(predicted.intersection(alignments))\n",
    "        total = len(alignments)\n",
    "\n",
    "        if name not in similarity_map:\n",
    "            similarity_map[name] = []\n",
    "\n",
    "        similarity_map[name].append((threshold,) + metrics(correct_count, predicted_count, total))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for k in similarity_map:\n",
    "    for t in similarity_map[k]:\n",
    "        metrics.append([k] + list(t))\n",
    "\n",
    "df = pd.DataFrame(metrics, columns=['name', 'threshold', 'precision', 'recall', 'f-measure'])\n",
    "df.to_csv('./nat/metrics.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "myenv",
   "language": "python",
   "display_name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
